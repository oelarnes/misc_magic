puzzle idea: binary code by left/right hand words

This post was brought on by some misconceptions about the Elo ratings system in another thread. I wanted to get out some information about how Elo really works for two reasons: First, everyone still has a ratings number in limited and it's useful to understand what it means. Second, I will argue that it's still the best system for measuring player skill in Magic, and those of you who are interested in using data analysis should understand what it purports to measure and what it doesn't.

Wall of text follows, tldr; Elo rating is an effective ratings system for random games, including Magic, it is not suitable as an invite system for pro play, and you should understand that your rating is designed to fluctuate around its true rating, not narrow in on it.

First, two misconceptions:

1)**Elo ratings is inappropriate for games with random elements.** In fact, the model depends on individual games closely resembling independent normal random variables (in a way I will describe later). The perfect game for Elo ratings would be a game consisting of throwing a pile of weighted dice.

2)**Elo ratings depend on assumptions about the distribution of skill across the player base.** I've seen this several times, and believed it myself until I actually learned the system, but it's not the case.
 
(disclaimer: The model described below is based on Elo's original ideas. The actual system used varies in each case, but has similar theoretical underpinnings.)

Ok, so what is the Elo rating actually measuring? First, we will apply it to a game I will call the Elo game. In the Elo game, each player is given a pile of dice by a powerful wizard, and the dice are weighted so that each player has a fixed average value that they will roll. The game is to find out how the average of your dice relates the other players' and develop a ranking system. The only way to find out is to sit down with another player, roll your dice, and see who gets the higher roll. You are not allowed to write down your total roll! At the end of the day, all the players have a bunch of win-loss records against other players, and want to try to puzzle out the true averages. 

To do this, we mathematically model games to figure out what exactly we are analyzing. Since the distribution of the sum of a pile of dice is approximately normal, we model the game as each player sampling from a normal distribution and comparing the result, as shown:

For the Elo game, this is a pretty good model. If we assume equal variance for all rolls (an important, but potentially problematic assumption), we have a mathematical model with a single unknown parameter for each player. That parameter is the theoretical average, and our estimate of that average will be the player's Elo rating. Since the comparison is only relative, and not absolute, we can pick an arbitrary mean value (say 1600). We also assign a number to the variance of individual dice rolls. We do NOT arbitrarily assign a value to the variance of averages among players. That will be determined relative to our scale.

If the averages were truly fixed, we would apply statistical tests like least squares or Bayesian inference (which would necessitate an estimate on the prior distribution), and after a lot of games we would only be tweaking our estimates. In reality, we want experienced players to have the same opportunity to increase their rating as new players, so we use a simpler model. The idea is that we are estimating ratings as we go, and after each game we update our ratings based on the results of the throw. If my ratings is 200 points higher than my opponent, I expect to win 76% of the time. If I lose, I lower my rating by an amount proportional to 1/.24, and if I win, I increase it proportional to 1/.76. Therefore my expected ratings change, if we are properly ranked, is zero. If we are not properly ranked, the ratings will tend to drift towards the correct ratings.

What *doesn't* happen is that the Elo ratings narrow in on your true skill rankings. Because of the fixed adjustment scheme, your rating will always fluctuate around your true rating no matter how much you play. A good choice of proportionality constant (K value) will ensure that the fluctuations are small in proportion to the difference between players, but big enough to allow players to improve their ratings in a reasonable amount of time.

The effectiveness of the Elo scheme, then, depends on how closely the mathematical model relates to game outcomes. If performances are truly like throwing huge piles of dice, then it will be work as intended. If something else is going on, there may be problems for wide differences in play skill, where the shape of the distribution really matters. There's not much of a difference between 76% and 74%, for example, but there is a huge difference between 1% and .01%. For example, if the system thought you had a 99.99% chance of beating a bad opponent, but in reality your chance was only 99%, playing that player would tend to decimate your rating.

Hopefully you can see that Elo is pretty much perfect for the Elo game. What about other games? Chess seems like an odd fit, as it hardly resembles throwing dice. However, Elo designed the system specifically for chess, so there must be something going on here. Elo looked at the data, and found that in fact it fits pretty well. Why might that be? We make three assumptions: 1) Each player makes a lot of decisions during the game, and roughly the same number of decisions per game. 2) Each decision is made independently, and its chance of success depends on the player's skill. 3) The player who makes the most correct decisions wins. With this model, your decisions start to look more like weighted dice, and the game outcome looks like throwing them all and comparing the total. Obviously we can point to lots of reasons chess doesn't actually work this way, and sure enough, at the margins of player skill, the model starts to break down. For example, good players may be more consistent in their decision making, which violates the assumption that all players have the same variance across performances. For this reason, different chess organizations have tweaked the Elo model to better suit their needs.

The extent to which a game is suited to Elo is the extent to which it satisfies the three assumptions. I would argue that Magic is somewhere between the dice game and chess in this respect. You can think of the deck as being an independent agent which makes decisions for you.  It makes the "correct" decision when it gives you a good card, and an "incorrect" choice when it delivers a bad card. Now we have a system with a lot more truly random and independent events than chess, which is a good thing as for as Elo is concerned. Even better, since players come with more or less similar decks, the amount of variance is more likely to be consistent from player to player, which makes the model more likely to apply even at the outer bounds of player performance. A true defense of the applicability of Elo would depend on an analysis of large amounts of data, which I don't currently have available. If you are aware of a large data set suitable for this purpose, I would be interested in hearing from you!

Now, to address the issue of Wizards abandoning Elo in favor of various cumulative rankings. Despite everything I've said, this decision was absolutely correct. Let's look at the issue of protecting rankings. Consider the following simplified situation: There are two groups of players, a large group E of everyday players, and a small group P of pro players. The two groups do not generally play against each other. When a player from E breaks a certain ratings barrier (remember, rating is designed to fluctuate around its theoretical value), he moves into group P. 

Almost every time this happens, they are "stealing"  some points from group E and moving them to group P. For instance, imagine you went on a 25-2 run over several events. Your rating takes those results more heavily into account than your previous results and your rating is therefore higher than your actual skill. So yes, you are a skilled player, and yes, you deserve to go to the PT, but you are still stealing some points from E and sending them to P. Now you are playing in a pool of players with an inflated average rating. You can happily play in group P and not worry about your rating reverting to its true value. The problem is you are disincentivized from playing group E, since that group has a deflated mean and will eventually steal your points back. So not only are pro players discouraged from playing in everyday events, they are actually stealing points from the population and making it harder for new players to break in!

Despite the problems, I believe there are still valid applications of Elo ratings to Magic today. With large, fluid groups of players, it is a method with a solid basis in mathematics to estimate your chances of winning against fields of various skill. For example, if you knew the ratings distribution for players entering 84, 4322, swiss, and sealed events, respectively, you could make more accurate expected value calculations, even taking into account the increase in player skill across rounds of an elimination tournament.

I'll end with a note about applying your rating to your own play skill. Since it takes recent games more heavily into account, it does not represent the best possible estimate of your skill. If you think your skill has remained reasonably constant over a period of a year, you can simply average your rating across that period to obtain a less volatile estimate. The common mistake is to hit a new high rating, then latch onto that as "your" rating. Inevitably, your rating declines from that peak, but in your mind that's your number and you just need to stop running bad so you can get back up to it. You will be better served by averaging your ratings over some period of time and properly evaluating your skill.

Hopefully you've found this article useful, but I've said a number of controversial things and I expect to be flamed a bit. If you choose to do so, I just ask that you take the time to properly research the Elo model and point out my mistakes with well-sourced arguments. Thanks for reading!